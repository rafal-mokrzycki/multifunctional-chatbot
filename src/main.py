import datetime
import os

import repackage
import tiktoken
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.schema import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import OllamaLLM
from langchain_openai import ChatOpenAI
from openai import OpenAI
from pinecone import Pinecone

repackage.up()
from cfg.config import load_config
from utils.constants import ConstantsManagement

CONFIG = load_config()
CONSTANTS = ConstantsManagement()
CLIENT = OpenAI(api_key=CONFIG["openai_api_key"])

llm = ChatOpenAI(api_key=CONFIG["openai_api_key"], model="gpt-4o-mini")


def initialize_pinecone() -> Pinecone.Index:
    """Initialize Pinecone Index

    Returns:
        Pinecone.Index: Pinecone Index
    """
    api_key = CONFIG["pinecone_api_key"]
    index_name = CONSTANTS.PINECONE_INDEX_NAME
    environment = CONSTANTS.PINECONE_ENVIRONMENT

    # Initialize Pinecone
    pc = Pinecone(api_key=api_key, environment=environment)
    return pc.Index(index_name)


index = initialize_pinecone()

template = """
Answer the question below.

Here is the conversation history: {context}

Question: {question}

Answer:
"""

model = OllamaLLM(model="llama3")
prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model


def retrieve_from_pinecone(query):
    """Retrieve relevant information from Pinecone based on the query."""
    query_embedding = generate_query_embedding(query)
    response = index.query(vector=query_embedding, top_k=5, include_metadata=True)
    return [match["metadata"]["text"] for match in response["matches"]]


def generate_query_embedding(query):
    """Generate an embedding for the given query."""
    # Tokenize the input query
    tokenizer = tiktoken.get_encoding(CONSTANTS.TIKTOKEN_ENCODING)
    inputs = tokenizer.encode(query)

    # Call OpenAI API to generate embeddings
    embedding_response = CLIENT.embeddings.create(
        input=inputs, model="text-embedding-3-small"
    )

    # Extract and return the embedding from the response
    return embedding_response.data[0].embedding


def is_specific_topic(query):
    """Determine if the query is for a specific topic."""
    return any(
        keyword.lower() in query.lower() for keyword in CONSTANTS.SPECIFIC_KEYWORDS
    )


def handle_conversation_decorator(func):
    """
    A decorator that allows a function to operate in both command-line
    interface (CLI) mode and web application mode.

    This decorator wraps a function that handles conversation logic. It
    checks whether the function is called with context and question
    parameters. If neither is provided, it enters CLI mode, where it
    prompts the user for input until "exit" is typed. In this mode,
    it maintains conversation history and prints responses from the
    wrapped function.

    If context and question are provided, it assumes web mode and
    calls the wrapped function directly with those parameters.

    Parameters:
    ----------
    func : callable
        The function to be decorated. This function should accept two
        parameters: `context` (str) and `question` (str), and return a
        response generated by the chatbot.

    Returns:
    -------
    callable
        The wrapper function that can handle both CLI and web modes.

    Example:
    --------
    @handle_conversation_decorator
    def chat_logic(context, question):
        # Logic to generate a response
        return response

    In CLI mode:
    >>> chat_logic()  # Starts interactive conversation in CLI.

    In web mode:
    >>> response = chat_logic("previous context", "What is your name?")
    >>> print(response)  # Outputs the chatbot's response.
    """

    def wrapper(context=None, question=None):
        if context is None and question is None:
            # CLI mode
            context = ""
            print("Welcome to the AI ChatBot! Type 'exit' to quit.")

            # Create a timestamp for the log file name
            # at the start of the conversation
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file_path = os.path.join("logs", f"conversation_{timestamp}.log")

            # Ensure logs directory exists
            if not os.path.exists("logs"):
                os.makedirs("logs")

            with open(log_file_path, "a", encoding="utf-8") as log_file:
                while True:
                    user_input = input("You: ")
                    if user_input.lower() == "exit":
                        break

                    result = func(context, user_input)
                    print("Bot: ", result)

                    # Write to the log file for each interaction
                    log_file.write(f"User: {user_input}\n")
                    log_file.write(f"AI: {result}\n")

                    # Update context with user input and bot response
                    context += f"\nUser: {user_input}\nAI: {result}"
        else:
            # Web mode
            return func(context, question)

    return wrapper


@handle_conversation_decorator
def handle_conversation(context, question, log_file_path=None):
    """
    Generate a response from the chatbot based on the provided context
    and question.

    This function takes the current conversation context and a user
    question, invokes the language model to generate a response, and
    returns the result.

    Parameters:
    ----------
    context : str
        The conversation history that provides context for generating a
        response.

    question : str
        The user's question or input that needs to be processed by the
        chatbot.

    Returns:
    -------
    str
        The response generated by the chatbot based on the provided
        context and question.

    Example:
    --------
    >>> response = handle_conversation("User: Hello\nAI: Hi there!",
    ... "How are you?")
    >>> print(response)  # Outputs the chatbot's response to the
    ... question.
    """
    if is_specific_topic(question):
        pinecone_results = retrieve_from_pinecone(question)
        if pinecone_results:
            # Generate a short answer from retrieved results
            result = generate_short_answer(pinecone_results)
        else:
            result = generate_default_response(question)
    else:
        result = chain.invoke({"context": context, "question": question})

    if log_file_path:
        with open(log_file_path, "a") as log_file:
            log_file.write(f"User: {question}\n")
            log_file.write(f"AI: {result}\n")
    return result


def generate_short_answer(results):
    """
    Generate a concise answer from retrieved results using Langchain.

    Parameters:
    ----------
    results : list
        List of results retrieved from Pinecone.

    Returns:
    -------
    str
        A short summary or key point derived from results.
    """
    # Convert strings to Document objects
    documents = [Document(page_content=result) for result in results]

    # Create a prompt template for summarization
    prompt_template = ChatPromptTemplate.from_messages(
        [("system", "Write a concise summary of the following:\n\n{context}")]
    )

    # Create the Stuff chain for summarization
    stuff_chain = create_stuff_documents_chain(llm, prompt_template)

    # Join the results into a single string for summarization
    context = "\n".join(results)

    # Run the chain to get the summary
    summary = stuff_chain.invoke({"context": documents})

    return summary


def generate_default_response(question):
    """
    Generate a default response based on general knowledge.

    Parameters:
    ----------
    question : str
        The user's question.

    Returns:
    -------
    str
        A valid answer generated from general knowledge.
    """
    # Implement logic to generate an answer based on general knowledge.
    return "I'm not sure about that. Can you please provide more details?"
