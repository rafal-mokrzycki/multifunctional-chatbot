import datetime
import os

from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import OllamaLLM

template = """
Answer the question below.

Here is the conversation history: {context}

Question: {question}

Answer:
"""

model = OllamaLLM(model="llama3")
prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model


def handle_conversation_decorator(func):
    """
    A decorator that allows a function to operate in both command-line
    interface (CLI) mode and web application mode.

    This decorator wraps a function that handles conversation logic. It
    checks whether the function is called with context and question
    parameters. If neither is provided, it enters CLI mode, where it
    prompts the user for input until "exit" is typed. In this mode,
    it maintains conversation history and prints responses from the
    wrapped function.

    If context and question are provided, it assumes web mode and
    calls the wrapped function directly with those parameters.

    Parameters:
    ----------
    func : callable
        The function to be decorated. This function should accept two
        parameters: `context` (str) and `question` (str), and return a
        response generated by the chatbot.

    Returns:
    -------
    callable
        The wrapper function that can handle both CLI and web modes.

    Example:
    --------
    @handle_conversation_decorator
    def chat_logic(context, question):
        # Logic to generate a response
        return response

    In CLI mode:
    >>> chat_logic()  # Starts interactive conversation in CLI.

    In web mode:
    >>> response = chat_logic("previous context", "What is your name?")
    >>> print(response)  # Outputs the chatbot's response.
    """

    def wrapper(context=None, question=None):
        if context is None and question is None:
            # CLI mode
            context = ""
            print("Welcome to the AI ChatBot! Type 'exit' to quit.")

            # Create a timestamp for the log file name
            # at the start of the conversation
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file_path = os.path.join("logs", f"conversation_{timestamp}.log")

            # Ensure logs directory exists
            if not os.path.exists("logs"):
                os.makedirs("logs")

            with open(log_file_path, "a", encoding="utf-8") as log_file:
                while True:
                    user_input = input("You: ")
                    if user_input.lower() == "exit":
                        break

                    result = func(context, user_input)
                    print("Bot: ", result)

                    # Write to the log file for each interaction
                    log_file.write(f"User: {user_input}\n")
                    log_file.write(f"AI: {result}\n")

                    # Update context with user input and bot response
                    context += f"\nUser: {user_input}\nAI: {result}"
        else:
            # Web mode
            return func(context, question)

    return wrapper


@handle_conversation_decorator
def handle_conversation(context, question, log_file_path=None):
    """
    Generate a response from the chatbot based on the provided context
    and question.

    This function takes the current conversation context and a user
    question, invokes the language model to generate a response, and
    returns the result.

    Parameters:
    ----------
    context : str
        The conversation history that provides context for generating a
        response.

    question : str
        The user's question or input that needs to be processed by the
        chatbot.

    Returns:
    -------
    str
        The response generated by the chatbot based on the provided
        context and question.

    Example:
    --------
    >>> response = handle_conversation("User: Hello\nAI: Hi there!",
    ... "How are you?")
    >>> print(response)  # Outputs the chatbot's response to the
    ... question.
    """
    result = chain.invoke({"context": context, "question": question})

    # If log_file_path is provided, log the conversation
    if log_file_path:
        with open(log_file_path, "a") as log_file:
            log_file.write(f"User: {question}\n")
            log_file.write(f"AI: {result}\n")

    return result  # Return the result instead of printing it


if __name__ == "__main__":
    handle_conversation()  # Run in CLI mode
